{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136692bb-4d2d-4cd2-8739-717508b4d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_runway import Discovery, GraphDataModeler, PyIngest, UserInput\n",
    "from neo4j_runway.code_generation import PyIngestConfigGenerator\n",
    "from neo4j_runway.llm.openai import OpenAIDiscoveryLLM, OpenAIDataModelingLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c37c46-f4ad-47be-87d7-6729c1c8dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_directory = \"/home/jovyan/Elastic_DS/csv_downloads/conn.csv\"\n",
    "\n",
    "# data_dictionary = {\n",
    "#                 '_index': 'unique id for index.',\n",
    "#                 '@timestamp': 'timestamp for event.',\n",
    "#                 'message': 'event message.',\n",
    "#                 'agent.type': 'elastic agent type ex. filebeat, auditbeat ect.',\n",
    "#                 'host.hostname': \"hostename.\",\n",
    "#                 'host.ip': 'host machine IP address.',\n",
    "#                 'host.mac': 'host machine mac address.',\n",
    "#                 'host.os.type': 'OS family ex. linux or windows.',\n",
    "#                 'host.os.version': 'distro version number.',\n",
    "#                 }\n",
    "\n",
    "# use_cases = [\n",
    "#         \"which event had falures in the mesage\",\n",
    "#         \"how many faild login attempts\",\n",
    "#     ]\n",
    "\n",
    "# data = load_local_files(data_directory=data_directory,\n",
    "#                         data_dictionary=data_dictionary,\n",
    "#                         general_description=\"This is zeek network data that will be used alongside filebeat for assosiation analysis.\",\n",
    "#                         use_cases=use_cases,\n",
    "#                         include_files=[\"output.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062f2ce-2490-4027-b500-c4f5a4d4dce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Ensure .env file contains 'OPENAI_API_KEY' and restart the script.\")\n",
    "\n",
    "# Initialize OpenAI client with the API key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Define model name (change this to switch models)\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "# Helper function to dynamically generate completion parameters based on the model\n",
    "def get_completion_params(model_name, prompt, tokens=700, temperature=0.1):\n",
    "    # Use 'max_completion_tokens' for o3-mini; otherwise, use 'max_tokens'\n",
    "    token_param = \"max_completion_tokens\" if model_name == \"o3-mini\" else \"max_tokens\"\n",
    "    params = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        token_param: tokens,\n",
    "    }\n",
    "    # Only include temperature if the model supports it\n",
    "    if model_name != \"o3-mini\":\n",
    "        params[\"temperature\"] = temperature\n",
    "    return params\n",
    "\n",
    "# Define CSV file location\n",
    "data_directory = \"/home/jovyan/Elastic_DS/csv_downloads/conn.csv\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(data_directory):\n",
    "    raise FileNotFoundError(f\"File not found: {data_directory}\")\n",
    "\n",
    "# Load CSV to extract column headers and sample data\n",
    "df = pd.read_csv(data_directory, nrows=50)  # Load first 50 rows for analysis\n",
    "columns_list = list(df.columns)\n",
    "sample_data = df.to_dict(orient=\"records\")[:3]  # Get a small sample of data\n",
    "print(\"CSV Columns:\", columns_list)\n",
    "\n",
    "# Function to safely extract JSON from GPT response\n",
    "def extract_json(text):\n",
    "    \"\"\"Extracts and validates JSON from the GPT response.\"\"\"\n",
    "    match = re.search(r'\\{.*\\}', text, re.DOTALL)  # Try regex extraction\n",
    "    if match:\n",
    "        json_str = match.group()\n",
    "    else:\n",
    "        raise ValueError(\"No valid JSON object detected in GPT response.\")\n",
    "    \n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON. GPT Output:\\n\", text)\n",
    "        raise e\n",
    "\n",
    "# Function to dynamically generate dataset context using GPT\n",
    "def generate_dynamic_context(columns, sample):\n",
    "    prompt = f\"\"\"I have a CSV file with the following column headers:\n",
    "{columns}\n",
    "Here is a small sample of the data:\n",
    "{json.dumps(sample, indent=2)}\n",
    "\n",
    "Analyze this data and generate:\n",
    "1. A general description of what this dataset represents.\n",
    "2. A list of 5 potential use cases relevant to analyzing this data.\n",
    "3. general description and usecases should be targeted at scoping an llm to look at potential assosiations in data.\n",
    "\n",
    "Output a JSON object with two keys: \"general_description\" (a single-line string) and \"use_cases\" (a list of short bullet points).\n",
    "Output only a complete valid JSON object (including the curly braces) without any markdown formatting or code fences.\n",
    "\"\"\"\n",
    "    params = get_completion_params(model_name, prompt, tokens=700, temperature=0.1)\n",
    "    response = client.chat.completions.create(**params)\n",
    "    context_str = response.choices[0].message.content.strip()\n",
    "    print(\"GPT Context Response:\\n\", context_str)  # Debugging output\n",
    "    return extract_json(context_str)\n",
    "\n",
    "# Generate dynamic description and use cases\n",
    "context_info = generate_dynamic_context(columns_list, sample_data)\n",
    "general_description = context_info[\"general_description\"]\n",
    "use_cases = context_info[\"use_cases\"]\n",
    "\n",
    "print(\"\\nGenerated General Description:\", general_description)\n",
    "print(\"Generated Use Cases:\", use_cases)\n",
    "\n",
    "# Function to generate data dictionary using GPT\n",
    "def generate_data_dictionary(columns, general_description, use_cases):\n",
    "    prompt = f\"\"\"I have a CSV file with the following column headers:\n",
    "{columns}\n",
    "The file contains network data.\n",
    "General description: \"{general_description}\"\n",
    "Use cases: {use_cases}\n",
    "Generate a JSON object that maps each column header to a concise description of what the column represents.\n",
    "Ensure the descriptions are tailored to this data context and are targeted and stearing an llm to building data assostiations.\n",
    "Output only a complete valid JSON object (including the curly braces) without any markdown formatting or code fences.\n",
    "\"\"\"\n",
    "    params = get_completion_params(model_name, prompt, tokens=700, temperature=0.1)\n",
    "    response = client.chat.completions.create(**params)\n",
    "    data_dict_str = response.choices[0].message.content.strip()\n",
    "    print(\"\\nGPT Data Dictionary Response:\\n\", data_dict_str)  # Debugging output\n",
    "    return extract_json(data_dict_str)\n",
    "\n",
    "# Generate the data dictionary using GPT, with error handling to define data_dictionary\n",
    "try:\n",
    "    data_dictionary = generate_data_dictionary(columns_list, general_description, use_cases)\n",
    "    print(\"\\nGenerated Data Dictionary:\", data_dictionary)\n",
    "except ValueError as e:\n",
    "    print(\"Failed to generate data dictionary:\", e)\n",
    "    data_dictionary = {}  # Set to an empty dictionary to avoid NameError\n",
    "\n",
    "# Function to load and structure local files\n",
    "def load_local_files(data_directory, data_dictionary, general_description, use_cases, include_files):\n",
    "    \"\"\"Loads and structures local files into a dictionary for further processing.\"\"\"\n",
    "    if not os.path.exists(data_directory):\n",
    "        raise FileNotFoundError(f\"File not found: {data_directory}\")\n",
    "\n",
    "    df = pd.read_csv(data_directory)  # Load full dataset\n",
    "    \n",
    "    # Package metadata and data\n",
    "    data = {\n",
    "        \"file_path\": data_directory,\n",
    "        \"columns\": list(df.columns),\n",
    "        \"data_dictionary\": data_dictionary,\n",
    "        \"general_description\": general_description,\n",
    "        \"use_cases\": use_cases,\n",
    "        \"sample_data\": df.head(10).to_dict(orient=\"records\"),\n",
    "        \"included_files\": include_files\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load and structure the dataset\n",
    "data = load_local_files(\n",
    "    data_directory=data_directory,\n",
    "    data_dictionary=data_dictionary,\n",
    "    general_description=general_description,\n",
    "    use_cases=use_cases,\n",
    "    include_files=[\"output.csv\"]\n",
    ")\n",
    "\n",
    "print(\"\\nLoaded Data Structure:\", json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32cc777-3ec1-4294-b7ba-0d4f4cacab41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def generate_general_description(df):\n",
    "    \"\"\"\n",
    "    Generates a general description of the DataFrame by providing key insights\n",
    "    such as the number of rows and the list of columns.\n",
    "    \"\"\"\n",
    "    columns_list = list(df.columns)\n",
    "    num_rows = len(df)\n",
    "    prompt = f\"\"\"I have a dataframe with {num_rows} rows and the following columns: {columns_list}.\n",
    "Please provide a concise general description of this dataset focusing on its key attributes, potential applications, and any notable insights.\n",
    "Output only the description text with no additional commentary.\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"gpt-4o\",\n",
    "        max_tokens=150,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    description = response.choices[0].message.content.strip()\n",
    "    return description\n",
    "\n",
    "def load_local_files(data_directory, data_dictionary, use_cases, include_files):\n",
    "    try:\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(data_directory)\n",
    "        \n",
    "        # Rename columns based on the data dictionary if necessary\n",
    "        # (Here, the mapping is identity, but adjust if needed.)\n",
    "        df.rename(columns={k: k for k in data_dictionary.keys()}, inplace=True)\n",
    "        \n",
    "        # Generate general description using GPT‚Äë4o based on the DataFrame\n",
    "        general_description = generate_general_description(df)\n",
    "        \n",
    "        # Print summary information\n",
    "        print(f\"Loaded data from {data_directory}\")\n",
    "        print(f\"General Description: {general_description}\")\n",
    "        print(f\"Available Use Cases: {use_cases}\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Now you can call the function\n",
    "data = load_local_files(data_directory=data_directory,\n",
    "                        data_dictionary=data_dictionary,\n",
    "                        use_cases=use_cases,\n",
    "                        include_files=[\"/csv_downloads/conn.csv\"])\n",
    "\n",
    "# Display first few rows\n",
    "if data is not None:\n",
    "    print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41019b6c-0298-4a8c-a81f-4e74be3328d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Manually set the API key in the environment (if not already set)\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize LLMs (without api_key argument)\n",
    "llm_disc = OpenAIDiscoveryLLM(\n",
    "    model_name='gpt-4o',\n",
    "    model_params={\"temperature\": 0}\n",
    ")\n",
    "\n",
    "llm_dm = OpenAIDataModelingLLM(\n",
    "    model_name='gpt-4o',\n",
    "    model_params={\"temperature\": 0.5}\n",
    ")\n",
    "\n",
    "# Verify the API key is available\n",
    "print(f\"API Key Loaded: {bool(os.getenv('OPENAI_API_KEY'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a469949-06d9-4d32-8325-0df0c36f483e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Discovery instance\n",
    "disc = Discovery(llm=llm_disc, data=data)\n",
    "\n",
    "# Run Discovery\n",
    "disc.run()\n",
    "\n",
    "# Run again with additional parameters\n",
    "disc.run(show_result=True, notebook=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a009481-f297-4752-8c86-6f617bb07ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdm = GraphDataModeler(llm=llm_dm, discovery=disc)\n",
    "gdm.create_initial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b783e8-1347-4abe-8aff-a50e0a6c5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize_label(node_obj):\n",
    "    \"\"\"\n",
    "    Converts a node object into a consistent, hashable identifier.\n",
    "    Tries node_obj.id, node_obj.name, or parses the string representation.\n",
    "    \"\"\"\n",
    "    # If there's a unique 'id' or 'name' attribute, use that\n",
    "    if hasattr(node_obj, 'id'):\n",
    "        return str(node_obj.id)  # Ensure it's a string\n",
    "    if hasattr(node_obj, 'name'):\n",
    "        return str(node_obj.name)\n",
    "    \n",
    "    # Fallback: parse the string representation, e.g. '(:Label)'\n",
    "    s = str(node_obj)\n",
    "    if s.startswith('(:') and s.endswith(')'):\n",
    "        # Strip off the '(:' and the closing ')'\n",
    "        return s[2:-1].strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "def visualize_graph(model):\n",
    "    \"\"\"Visualizes the model's graph structure using networkx and matplotlib.\"\"\"\n",
    "    G = nx.DiGraph()  # Create a directed graph\n",
    "\n",
    "    # Add nodes with normalized labels\n",
    "    for node in getattr(model, 'nodes', []):\n",
    "        node_id = normalize_label(node)\n",
    "        G.add_node(node_id)\n",
    "\n",
    "    # Add edges (if the model has relationships)\n",
    "    if hasattr(model, 'relationships'):\n",
    "        for rel in model.relationships:\n",
    "            src_obj = getattr(rel, 'source', getattr(rel, 'start', None))\n",
    "            dst_obj = getattr(rel, 'target', getattr(rel, 'end', None))\n",
    "            if src_obj and dst_obj:\n",
    "                src_id = normalize_label(src_obj)\n",
    "                dst_id = normalize_label(dst_obj)\n",
    "                G.add_edge(src_id, dst_id)\n",
    "\n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(\n",
    "        G, pos, \n",
    "        with_labels=True, \n",
    "        node_color=\"lightblue\", \n",
    "        edge_color=\"gray\",\n",
    "        node_size=2000, \n",
    "        font_size=10, \n",
    "        font_color=\"black\", \n",
    "        arrows=True\n",
    "    )\n",
    "    plt.title(\"Model Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "visualize_graph(gdm.current_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f6fb68-fda2-4fc6-975e-dda76189f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # üöÄ STEP 1: Load CSV & Clean Data\n",
    "# csv_path = \"/home/jovyan/Autogen/output.csv\"  # Update if needed\n",
    "# clean_csv_path = \"/home/jovyan/Autogen/output_clean.csv\"\n",
    "\n",
    "# # Load the CSV\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Normalize column names (strip spaces, lowercase)\n",
    "# df.columns = df.columns.str.strip()\n",
    "\n",
    "# # Rename columns to match Neo4j expectations\n",
    "# column_mapping = {\n",
    "#     \"agent.id\": \"agentId\",  # Fix agent ID reference\n",
    "#     \"host.id\": \"hostId\",  # Ensure hostId consistency\n",
    "#     \"log.file.device_id\": \"logFileDeviceId\",  # Standardize log file ID\n",
    "# }\n",
    "# df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# # Replace NaN/null with default values to avoid MERGE issues\n",
    "# df.fillna({\"agentId\": \"UNKNOWN_AGENT\"}, inplace=True)\n",
    "# df.fillna(\"\", inplace=True)\n",
    "\n",
    "# # Ensure agentId is a string\n",
    "# df[\"agentId\"] = df[\"agentId\"].astype(str)\n",
    "\n",
    "# # Save the cleaned CSV\n",
    "# df.to_csv(clean_csv_path, index=False)\n",
    "# print(\"‚úÖ Data cleaned and saved to:\", clean_csv_path)\n",
    "\n",
    "# # üöÄ STEP 2: Generate PyIngest YAML Configuration\n",
    "# gen = PyIngestConfigGenerator(\n",
    "#     data_model=gdm.current_model,\n",
    "#     uri=\"neo4j://192.168.2.2:7687\",\n",
    "#     database=\"neo4j\",\n",
    "#     file_directory=\"/\",\n",
    "#     source_name=\"output_clean.csv\"\n",
    "# )\n",
    "\n",
    "# pyingest_yaml = gen.generate_config_string()\n",
    "\n",
    "# # Fix incorrect column references in YAML\n",
    "# pyingest_yaml = pyingest_yaml.replace(\"row.agent.id\", \"row.agentId\")\n",
    "# pyingest_yaml = pyingest_yaml.replace(\"row.host.id\", \"row.hostId\")\n",
    "# pyingest_yaml = pyingest_yaml.replace(\"row.log.file.device_id\", \"row.logFileDeviceId\")\n",
    "\n",
    "# # Debugging: Print YAML to verify correct mappings\n",
    "# print(\"üîç Updated YAML Config:\\n\", pyingest_yaml)\n",
    "\n",
    "# # üöÄ STEP 3: Ingest Cleaned Data into Neo4j\n",
    "# PyIngest(config=pyingest_yaml, verbose=True)\n",
    "\n",
    "# print(\"‚úÖ Data successfully ingested into Neo4j!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ca648-cb13-497a-91e1-07e86d8bf874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml  # Requires PyYAML\n",
    "import time\n",
    "import threading\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client using the new interface\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# üöÄ STEP 1: Load CSV & Initial Clean\n",
    "csv_path = data_directory  # Update if needed\n",
    "\n",
    "# Define directory for cleaned CSV and ensure it exists.\n",
    "clean_dir = \"/home/jovyan/Elastic_DS/csv_downloads/clean\"\n",
    "os.makedirs(clean_dir, exist_ok=True)\n",
    "if not clean_dir.endswith(os.sep):\n",
    "    clean_dir += os.sep\n",
    "\n",
    "# Define the full path for the cleaned CSV file.\n",
    "clean_csv_file = os.path.join(clean_dir, \"output_clean.csv\")\n",
    "\n",
    "# Load the CSV and normalize column names\n",
    "df = pd.read_csv(csv_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# üöÄ STEP 2: Dynamically Generate Column Mapping using GPT‚Äë4o\n",
    "def generate_column_mapping(columns):\n",
    "    prompt = f\"\"\"I have a list of CSV column names:\n",
    "{columns}\n",
    "Generate a JSON object mapping each column name to a new column name following Neo4j's camelCase naming convention.\n",
    "Output only a complete JSON object (including the opening and closing curly braces) and nothing else.\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"gpt-4o\",\n",
    "        max_tokens=300,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    mapping_str = response.choices[0].message.content.strip()\n",
    "    match = re.search(r'\\{.*\\}', mapping_str, re.DOTALL)\n",
    "    if match:\n",
    "        mapping_str = match.group()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid JSON response from GPT‚Äë4o\")\n",
    "    \n",
    "    try:\n",
    "        mapping = json.loads(mapping_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise e\n",
    "    return mapping\n",
    "\n",
    "columns_list = list(df.columns)\n",
    "column_mapping = generate_column_mapping(columns_list)\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# --- Dynamic DataFrame Cleaning Function ---\n",
    "def clean_dataframe(df):\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = df[col].apply(lambda x: 0 if pd.isna(x) or (isinstance(x, float) and np.isnan(x)) or x == \"\" else x)\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(pd.Timestamp(\"1970-01-01\"))\n",
    "        else:\n",
    "            df[col] = df[col].replace({None: \"UNKNOWN\", pd.NA: \"UNKNOWN\"}).fillna(\"UNKNOWN\")\n",
    "            df[col] = df[col].astype(str).apply(lambda x: \"UNKNOWN\" if x.lower() in [\"nan\", \"none\", \"null\", \"\"] else x)\n",
    "    return df\n",
    "\n",
    "df = clean_dataframe(df)\n",
    "df.fillna(\"UNKNOWN\", inplace=True)\n",
    "\n",
    "# üöÄ STEP 3: Generate PyIngest YAML Configuration\n",
    "gen = PyIngestConfigGenerator(\n",
    "    data_model=gdm.current_model,\n",
    "    uri=\"neo4j://192.168.2.2:7687\",\n",
    "    database=\"neo4j\",\n",
    "    file_directory=clean_dir,       # clean_dir ends with a slash\n",
    "    source_name=\"output_clean.csv\"\n",
    ")\n",
    "pyingest_yaml = gen.generate_config_string()\n",
    "\n",
    "# Replace column references dynamically.\n",
    "for original, new in column_mapping.items():\n",
    "    pyingest_yaml = pyingest_yaml.replace(f\"row.{original}\", f\"row.{new}\")\n",
    "\n",
    "# Parse YAML and fix file URLs.\n",
    "config_dict = yaml.safe_load(pyingest_yaml)\n",
    "if \"files\" in config_dict:\n",
    "    for file_entry in config_dict[\"files\"]:\n",
    "        if \"url\" in file_entry:\n",
    "            url = file_entry[\"url\"].replace(\"$BASE\", \"\")\n",
    "            url = re.sub(r'^/+', '/', url)  # collapse multiple leading slashes\n",
    "            file_entry[\"url\"] = url\n",
    "pyingest_yaml = yaml.dump(config_dict)\n",
    "\n",
    "def ensure_dynamic_required_columns(df, yaml_str):\n",
    "    cols_in_yaml = re.findall(r\"row\\.([a-zA-Z0-9_.]+)\", yaml_str)\n",
    "    for col in set(cols_in_yaml):\n",
    "        if col not in df.columns:\n",
    "            df[col] = f\"UNKNOWN_{col.replace('.', '_').upper()}\"\n",
    "    return df\n",
    "\n",
    "df = ensure_dynamic_required_columns(df, pyingest_yaml)\n",
    "df = clean_dataframe(df)\n",
    "df.fillna(\"UNKNOWN\", inplace=True)\n",
    "\n",
    "def final_clean_value(x):\n",
    "    if pd.isna(x):\n",
    "        return \"UNKNOWN\"\n",
    "    try:\n",
    "        s = str(x).strip()\n",
    "    except Exception:\n",
    "        return \"UNKNOWN\"\n",
    "    return \"UNKNOWN\" if s.lower() in [\"\", \"nan\", \"none\", \"null\"] else s\n",
    "\n",
    "df = df.apply(lambda col: col.map(final_clean_value))\n",
    "df.replace(r'^\\s*$', \"UNKNOWN\", regex=True, inplace=True)\n",
    "df.fillna(\"UNKNOWN\", inplace=True)\n",
    "\n",
    "# Save the cleaned CSV\n",
    "df.to_csv(clean_csv_file, index=False)\n",
    "\n",
    "# üöÄ STEP 4: Ingest Cleaned Data into Neo4j with a Progress Bar\n",
    "\n",
    "# Calculate an estimated total number of batches.\n",
    "chunk_size = 100\n",
    "num_batches_per_file = math.ceil(len(df) / chunk_size)\n",
    "num_file_entries = len(config_dict.get(\"files\", []))\n",
    "total_batches = num_batches_per_file * num_file_entries\n",
    "\n",
    "def run_ingestion():\n",
    "    PyIngest(config=pyingest_yaml, verbose=False)\n",
    "\n",
    "print(\"Starting ingestion into Neo4j...\")\n",
    "ingest_thread = threading.Thread(target=run_ingestion)\n",
    "ingest_thread.start()\n",
    "\n",
    "progress = 0\n",
    "# Use a tqdm progress bar with the calculated total batches.\n",
    "with tqdm(total=total_batches, desc=\"Ingesting\", bar_format=\"{l_bar}{bar} [ elapsed: {elapsed} ]\") as pbar:\n",
    "    # Simulate progress updates.\n",
    "    while ingest_thread.is_alive():\n",
    "        time.sleep(0.2)\n",
    "        # Update progress by a fixed increment until we reach total_batches - 1.\n",
    "        if progress < total_batches - 1:\n",
    "            progress += 1\n",
    "            pbar.update(1)\n",
    "    # Ensure the progress bar is complete.\n",
    "    pbar.n = total_batches\n",
    "    pbar.refresh()\n",
    "\n",
    "ingest_thread.join()\n",
    "print(\"‚úÖ Data successfully ingested into Neo4j!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84663e33-4cab-4f7f-a04b-d5ad64012cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
